<html>
<head>
<title>CS 385 Final Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="../html/styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="../html/highlighting/styles/default.css">
<script src="../html/highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
  <h1>Seeing Double <span style="color: #DE3737">Jeff Booher-Kaeding, Ben Richardson, Trent Rhodes-ousley</span>&nbsp;</h1>
</div>
</div>
<div class="container">

<h2>CS 385 : Stereoscopic images in image classifiers </h2>

<div style="float: right; padding: 20px">
<img src="../html/Images/motorcycle_hsDMv.png" width="436" />
<p style="font-size: 14px">RGB represntation of a processed image</p>
</div>

<p> Image segmentation can operate fairly effectively on just 3 color channels. the Goal of this project was to add another channel dedicated to the distance of portions of the image from the camera. This required us to create a depth map by calculating the parallax between matched features in images taken from two parallel vantage points.
From this we would include the depth map into the semantic texton forest classifier in an attempt to improve accuracy of distinct classifications.
While the ultimate goal of the project was not achieved, many of the components came together into useful tools.
</p>
<div style="clear:both">

<h3>Goals of the project</h3>
<ol>
<li>To create a functional depth map using two images</li>
<li>Implement into a high-dimensional classifier</li>
<li>To compare the results with and without our depth map</li>
</ol>

<div style="clear:both">
<h3>Depth map</h3>

<p> Feature	maping:</p>
<ul>
  <li dir="ltr">
    <p dir="ltr">Sift feature identification</p>
  </li>
  <li dir="ltr">
    <p dir="ltr">vl_ubcmatch matches features based on the nearest neighbor in the other image</p>
  </li>
  <li dir="ltr">
    <p dir="ltr">These features can be culled based on:</p>
  </li>
  <ul>
    <li dir="ltr">
      <p dir="ltr">angle of deflection (~0.7 standard deviations)</p>
    </li>
    <li dir="ltr">
      <p dir="ltr">deflection far from the norm (~3 standard deviations)</p>
    </li>
  </ul>
</ul>
<p>&nbsp;</p>

<h2>Code for generating measures by which to exclude bad matches</h2>
<p>
The following code generates the angle and length of lines connecting  the matched points between the two images. It generates the distance using the distance formula, and the angle using trigonometry. These features are later used to cull angle and distance outliers, and generate the distance map.
</p>
<p>

<pre><code>m1= fa(1:2,matches(1,:));
m2=fb(1:2,matches(2,:));
%Stores the x and y location of the features deltax = m1(1,:) - m2(1,:);
deltay = m1(2,:) - m2(2,:) ; distances= sqrt((deltax).^2 + (deltay).^2);
angles= atan(deltay./deltax);  meanAngle= mean(angles); angleStdDeviation= std(angles);
MatchedPairs= [m1; m2; distances; angles] ;

</code></pre>

<h3>&nbsp;</h3>
<h2>Code for generating point depth</h2>
<p>The following code generates the depth of all of the matched points by using the distance between the matched points to calculate parallax. This parallax calculation uses an approximation common in astronomy: D = 1/p (where p is parallax, and D is distance).</p>
<p>The point set is also set up for interpolation by adding four corner pieces with an arbitrary depth value. This ensures that no portions of the resultant depth image are left null.</p>
<p>
<pre><code>%we will approximate distance: D = 1/p where p is the parallax angle
% 
p = c/2 ; %parallax is half of the angle c 
D = p.^-1 ; %This generates the distance from parallax 
D = (D/200)./((D/200)+1) ; %This maps an infinite distance range to 0,1 
Features3D= [MatchedPairs(1:2, :); D] ; %make an array of 3d points 
maxD = max(D); %find the max distance value, to set the corner points to. 
Features3D= [Features3D, [0;0;maxD], [0;size(Ia,1);maxD], [size(Ia,2);0;maxD], [size(Ia,2);size(Ia,1);maxD]]; 
%add corner points, set to maxD, so that no null regions are generated by 
%the mesh.

</code></pre>

<p>&nbsp;</p>
<h3>Results in a table</h3>

<table width="1000" border=1>
<tr>
<td width="10" height="100">
<img src="Images/plain01.jpg" width="300"/>
<img src="Images/plain02.jpg"  width="300"/>
<img src="Images/plain03.jpg" width="300"/>
</td>
</tr>

<tr>
<td width="10" height="100">
<img src="Images/unculled01.jpg" width="300"/>
<img src="Images/unculled02.jpg"  width="300"/>
<img src="Images/unculled03.jpg" width="300"/>
</td>
</tr>

<tr>
<td width="10" height="100">
<img src="Images/culledonangle01.jpg" width="300"/>
<img src="Images/culledonangle02.jpg"  width="300"/>
<img src="Images/culledonangle03.jpg" width="300"/>
</td>
</tr>

<tr>
<td width="10" height="100">
<img src="Images/culledonangleanddist01.jpg" width="300"/>
<img src="Images/culledonangleanddist02.jpg"  width="300"/>
<img src="Images/culledonangleanddist03.jpg" width="300"/>
</td>
</tr>

<tr>
<td width="10" height="100">
<img src="Images/depth01.jpg" width="300"/>
<img src="Images/depth02.jpg"  width="300"/>
<img src="Images/depth03.jpg" width="300"/>
</td>
</tr>

<tr>
<td width="10" height="100">
<img src="Images/hDMv01.jpg" width="300"/>
<img src="Images/hDMv02.jpg"  width="300"/>
<img src="Images/hDMv03.jpg" width="300"/>
</td>
</tr>

</table>

<p>&nbsp;</p>
<p>&nbsp;</p>

<h2>Semantic texton forest</h2>
<p> We used a classifer called semantic texton forest. Its made up of several texton probability trees with labels. 
 </p>
 <p>&nbsp;</p>
 

<h3>A Problem</h3>
<p>While it should be easy to add or reduce the dimensionality of the textons in theory, the code we used was not very adaptable. As it turns out the STF algorithm we used converted the images into a color format called lab color. We spent countless hours trying to implement an extended color channel to no avail.
 </p>
 
 <h3> A solution</h3>
 <ol>
 <li>Rather than try and to create a new channel for the STF. black box, we decided to add the height map to the images.</li>
 <li>We converted the RGB representation into HSV. Compressed two of the channels into one, and added the heightmap into one of the two condsend channel</li>
 </ol>
 <p>&nbsp;</p>
 
<h2>Code for generating our modifed HSV image</h2>
The following code generates a modifed HSV format image that truncates the hue, and the saturation into a single channel, then it fills our third channel with a depth map
<pre><code>img = rgb2hsv(image);%conver image to HSV


%% 
%compress hue and saturation into one channel 
sat = img(:, :, 2); %pull the saturation 
hue = img(:, :, 1); %pull the hue 
avg = (sat + hue)/2; %the the result and divide them by two 
img(:, :, 1) = avg; %assign the new average into the first channel 

%% 
%filled with zeros until we have the depth map. 
x = size(img(:,:,1)); %filling a 2-d array with zeros 
img(:, :, 2) = double(zeros(x)); %replace the saturation layer with our dept hmap 

%% 
%add in our depth map 
img(:, :, 2) = dm
</code></pre>

<p>&nbsp;</p>
<center>
<h2> RGB representation of HSV to hsV+dm</h2>
<p>RGB repreentation of HSV<p>
<img src="Images/hsv03.jpg">
<p> RGB represntation of HSV with the hue and saturation crushed into a single channel
<p>
<img src="Images/hv03.jpg">
<p>
Depth map to be added
<p>
<img src="Images/depth03.jpg">
<p>
RGB representation of hsV+dm
<p>
<img src="Images/hDMv03.jpg">
</center>
<p>

<h3>Takeaways</h3>
<div style="clear:both" >
<p>
	This project was hampered by a late database selection. We built the tools without having established something on which we could use these tools. Future projects should establish what the inputs and outputs will be early in development.
</p>
<p>
	We also learned that when other people's work is encorporated into our work, it is best to respect the encapsulation of that project, and focus on adapting the inputs and outputs to suit our needs rather than attempt to alter the project's inner workings. We spent countless hours tinkering with the inner workings of Semantic Texton Forest implementation, when we eventually settled on just altering its inputs. 
</p>
<p>
	The usefulness of a tool is often highly dependent on the nature of the input. Matlab's triangulate function would have given us a better quality heightmap than our hand implementation, but it required data we lacked or lacked the experiance to configure. 
</p>
<p>&nbsp;</p>

<h2>Conclusions</h2>
<div style="clear:both" >
<p> 
	
	We developed the tools needed to generate rough distance images from binocular sets, the tools needed to feed that information to a classifier, and enough of an understanding of the classifier to run it given a label set. However, we were unable to integrate the preprocessing and classification portions of the project in order to incorporate depth into image segmentation. This was in part due to a lack of a dataset that included a label set as well as binocular images, as well as a compressed window of time in which we had available to work within. Given another week, we believe we could have a working implementation, as a label set could be hand generated.
</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h4>References:</h4>
<l>
	<li>http://vision.middlebury.edu/stereo/data/scenes2014/</li>
	<li>http://machinelearning.ru/wiki/images/0/0a/Semantic_texton_forests.pdf</li>
	<li>https://www.mathworks.com/help/vision/examples/structure-from-motion-from-two-views.html</li>
</l>
</div>
</body>
</html>
